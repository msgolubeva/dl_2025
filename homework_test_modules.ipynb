{"cells":[{"cell_type":"code","execution_count":223,"metadata":{"collapsed":true,"executionInfo":{"elapsed":8,"status":"ok","timestamp":1743373726958,"user":{"displayName":"Dmitry Sokolov","userId":"07962866469303797928"},"user_tz":-180},"id":"fJIu9zDXqcdw"},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","import numpy as np\n","import unittest"]},{"cell_type":"code","execution_count":279,"metadata":{"executionInfo":{"elapsed":1019,"status":"ok","timestamp":1743380639090,"user":{"displayName":"Dmitry Sokolov","userId":"07962866469303797928"},"user_tz":-180},"id":"XvLelUBpqcdy"},"outputs":[],"source":["class TestLayers(unittest.TestCase):\n","    def test_Linear(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in, n_out = 2, 3, 4\n","        for _ in range(1):\n","            # layers initialization\n","            torch_layer = torch.nn.Linear(n_in, n_out)\n","            custom_layer = Linear(n_in, n_out)\n","            custom_layer.W = torch_layer.weight.data.numpy()\n","            custom_layer.b = torch_layer.bias.data.numpy()\n","\n","            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","            # 3. check layer parameters grad\n","            custom_layer.accGradParameters(layer_input, next_layer_grad)\n","            weight_grad = custom_layer.gradW\n","            bias_grad = custom_layer.gradb\n","            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n","            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n","\n","\n","    def test_SoftMax(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.Softmax(dim=1)\n","            custom_layer = SoftMax()\n","\n","            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n","            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n","            next_layer_grad = next_layer_grad.clip(1e-5,1.)\n","            next_layer_grad = 1. / next_layer_grad\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n","\n","    def test_LogSoftMax(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.LogSoftmax(dim=1)\n","            custom_layer = LogSoftMax()\n","\n","            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n","            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_BatchNormalization(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 32, 16\n","        for _ in range(100):\n","            # layers initialization\n","            slope = np.random.uniform(0.01, 0.05)\n","            alpha = 0.9\n","            custom_layer = BatchNormalization(alpha)\n","            custom_layer.train()\n","            torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n","            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n","            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n","\n","            # 3. check moving mean\n","            self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))\n","            # we don't check moving_variance because pytorch uses slightly different formula for it:\n","            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n","            #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n","\n","            # 4. check evaluation mode\n","            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","            custom_layer.evaluate()\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            torch_layer.eval()\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","    def test_Sequential(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            alpha = 0.9\n","            torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n","            torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n","            custom_layer = Sequential()\n","            bn_layer = BatchNormalization(alpha)\n","            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n","            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","            custom_layer.add(bn_layer)\n","            scaling_layer = ChannelwiseScaling(n_in)\n","            scaling_layer.gamma = torch_layer.weight.data.numpy()\n","            scaling_layer.beta = torch_layer.bias.data.numpy()\n","            custom_layer.add(scaling_layer)\n","            custom_layer.train()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            # print(torch_layer_output_var.data.numpy()[0][0], custom_layer_output[0][0], sep=\"==\")\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            # print(torch_layer_grad_var.data.numpy()[0][0], custom_layer_grad[0][0], sep=\" == \")\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-4))\n","\n","            # 3. check layer parameters grad\n","            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n","            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n","            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n","\n","    def test_Dropout(self):\n","        np.random.seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            p = np.random.uniform(0.3, 0.7)\n","            layer = Dropout(p)\n","            layer.train()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0),\n","                                        np.isclose(layer_output*(1.-p), layer_input))))\n","\n","            # 2. check layer input grad\n","            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n","            self.assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0),\n","                                        np.isclose(layer_grad*(1.-p), next_layer_grad))))\n","\n","            # 3. check evaluation mode\n","            layer.evaluate()\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.allclose(layer_output, layer_input))\n","\n","            # 4. check mask\n","            p = 0.0\n","            layer = Dropout(p)\n","            layer.train()\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.allclose(layer_output, layer_input))\n","\n","            p = 0.5\n","            layer = Dropout(p)\n","            layer.train()\n","            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","            layer_output = layer.updateOutput(layer_input)\n","            zeroed_elem_mask = np.isclose(layer_output, 0)\n","            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n","            self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n","\n","            # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n","            batch_size, n_in = 1000, 1\n","            p = 0.8\n","            layer = Dropout(p)\n","            layer.train()\n","\n","            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n","\n","            layer_input = layer_input.T\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n","\n","    def test_Conv2d(self):\n","        hyperparams = [\n","            {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n","             'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'},\n","            # {'batch_size': 4, 'in_channels': 1, 'out_channels': 2, 'height': 28, 'width': 28,\n","            #  'kernel_size': 5, 'stride': 2, 'padding': 2, 'bias': False, 'padding_mode': 'replicate'},\n","            # {'batch_size': 16, 'in_channels': 3, 'out_channels': 3, 'height': 64, 'width': 64,\n","            #  'kernel_size': 3, 'stride': 2, 'padding': 'same', 'bias': True, 'padding_mode': 'reflect'},\n","            # {'batch_size': 2, 'in_channels': 3, 'out_channels': 8, 'height': 10, 'width': 10,\n","            #  'kernel_size': 2, 'stride': (1,2), 'padding': 0, 'bias': True, 'padding_mode': 'zeros'},\n","        ]\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        for _ in range(3):\n","          for params in hyperparams:\n","              with self.subTest(params=params):\n","\n","                  batch_size = params['batch_size']\n","                  in_channels = params['in_channels']\n","                  out_channels = params['out_channels']\n","                  height = params['height']\n","                  width = params['width']\n","                  kernel_size = params['kernel_size']\n","                  stride = params['stride']\n","                  padding = params['padding']\n","                  bias = params['bias']\n","                  padding_mode = params['padding_mode']\n","\n","                  custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n","                                        stride=stride, padding=padding, bias=bias,\n","                                        padding_mode=padding_mode)\n","                  custom_layer.train()\n","\n","                  torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n","                                                stride=stride, padding=padding, bias=bias,\n","                                                padding_mode=padding_mode)\n","\n","                  custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n","                  if bias:\n","                      custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n","\n","                  layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n","                  input_var = torch.tensor(layer_input, requires_grad=True)\n","\n","                  custom_output = custom_layer.updateOutput(layer_input)\n","                  torch_output = torch_layer(input_var)\n","                  self.assertTrue(\n","                      np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","                  next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","                  custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","                  torch_output.backward(torch.tensor(next_layer_grad))\n","                  torch_grad = input_var.grad.detach().numpy()\n","                  # print(torch_grad[0][0][0], custom_grad[0][0][0], sep=\" == \")\n","                  self.assertTrue(\n","                      np.allclose(torch_grad, custom_grad, atol=1e-5))\n","\n","\n","    def test_LeakyReLU(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            slope = np.random.uniform(0.01, 0.05)\n","            torch_layer = torch.nn.LeakyReLU(slope)\n","            custom_layer = LeakyReLU(slope)\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_ELU(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            alpha = 1.0\n","            torch_layer = torch.nn.ELU(alpha)\n","            custom_layer = ELU(alpha)\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_SoftPlus(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.Softplus()\n","            custom_layer = SoftPlus()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_ClassNLLCriterionUnstable(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.NLLLoss()\n","            custom_layer = ClassNLLCriterionUnstable()\n","\n","            layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n","            layer_input /= layer_input.sum(axis=-1, keepdims=True)\n","            layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n","            target_labels = np.random.choice(n_in, batch_size)\n","            target = np.zeros((batch_size, n_in), np.float32)\n","            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(torch.log(layer_input_var),\n","                                                 Variable(torch.from_numpy(target_labels), requires_grad=False))\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n","            torch_layer_output_var.backward()\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_ClassNLLCriterion(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.NLLLoss()\n","            custom_layer = ClassNLLCriterion()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n","            target_labels = np.random.choice(n_in, batch_size)\n","            target = np.zeros((batch_size, n_in), np.float32)\n","            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var,\n","                                                 Variable(torch.from_numpy(target_labels), requires_grad=False))\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n","            torch_layer_output_var.backward()\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","\n","    def test_MaxPool2d(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, channels, height, width = 4, 3, 16, 16\n","        kernel_size, stride, padding = 2, 2, 0\n","\n","        for _ in range(100):\n","          custom_module = MaxPool2d(kernel_size, stride, padding)\n","          custom_module.train()\n","\n","          torch_module = torch.nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n","\n","          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n","          input_var = torch.tensor(input_np, requires_grad=True)\n","\n","          custom_output = custom_module.updateOutput(input_np)\n","          torch_output = torch_module(input_var)\n","          self.assertTrue(\n","              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","          torch_output.backward(torch.tensor(next_grad))\n","          torch_grad = input_var.grad.detach().numpy()\n","          self.assertTrue(\n","              np.allclose(torch_grad, custom_grad, atol=1e-5))\n","\n","    def test_AvgPool2d(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, channels, height, width = 4, 3, 16, 16\n","        kernel_size, stride, padding = 3, 2, 1\n","\n","        for _ in range(100):\n","          custom_module = AvgPool2d(kernel_size, stride, padding)\n","          custom_module.train()\n","\n","          torch_module = torch.nn.AvgPool2d(kernel_size, stride=stride, padding=padding)\n","\n","          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n","          input_var = torch.tensor(input_np, requires_grad=True)\n","\n","          custom_output = custom_module.updateOutput(input_np)\n","          torch_output = torch_module(input_var)\n","          self.assertTrue(\n","              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","          torch_output.backward(torch.tensor(next_grad))\n","          torch_grad = input_var.grad.detach().numpy()\n","          self.assertTrue(\n","              np.allclose(torch_grad, custom_grad, atol=1e-4))\n","\n","    def test_Flatten(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        test_params = [\n","            {'start_dim': 1, 'end_dim': -1},\n","            {'start_dim': 2, 'end_dim': 3},\n","            {'start_dim': 0, 'end_dim': -1},\n","        ]\n","\n","        for _ in range(100):\n","          for params in test_params:\n","              with self.subTest(params=params):\n","                  start_dim = params['start_dim']\n","                  end_dim = params['end_dim']\n","\n","                  custom_module = Flatten(start_dim, end_dim)\n","                  input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)\n","                  input_var = torch.tensor(input_np, requires_grad=True)\n","\n","                  custom_output = custom_module.updateOutput(input_np)\n","                  torch_output = torch.flatten(input_var, start_dim=start_dim, end_dim=end_dim)\n","                  self.assertTrue(\n","                      np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","                  next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","                  custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","                  torch_output.backward(torch.tensor(next_grad))\n","                  torch_grad = input_var.grad.detach().numpy()\n","                  self.assertTrue(\n","                      np.allclose(torch_grad, custom_grad, atol=1e-6))\n","\n","    def test_Gelu(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        for _ in range(100):\n","          custom_module = Gelu()\n","          custom_module.train()\n","\n","          torch_module = torch.nn.GELU()\n","\n","          input_np = np.random.randn(10, 5).astype(np.float32)\n","          input_var = torch.tensor(input_np, requires_grad=True)\n","\n","          custom_output = custom_module.updateOutput(input_np)\n","          torch_output = torch_module(input_var)\n","          self.assertTrue(\n","              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","          torch_output.backward(torch.tensor(next_grad))\n","          torch_grad = input_var.grad.detach().numpy()\n","          self.assertTrue(\n","              np.allclose(torch_grad, custom_grad, atol=1e-5))"]},{"cell_type":"code","execution_count":292,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1114,"status":"ok","timestamp":1743380802372,"user":{"displayName":"Dmitry Sokolov","userId":"07962866469303797928"},"user_tz":-180},"id":"eS9nJ-aHl0Q9","outputId":"1c24485a-5eea-430e-c9cb-7a93be5a664c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zuw3jfmiJZmd","outputId":"1422be74-2d5f-49e4-d7a0-2d704227076d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DL\n","X shape: (10000, 5)\n","y shape: (10000, 3)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-293-42e91effc014>:26: RuntimeWarning: overflow encountered in multiply\n","  self.moving_mean = self.alpha * self.moving_mean + (1 - self.alpha) * self.batch_mean\n","<ipython-input-293-42e91effc014>:27: RuntimeWarning: overflow encountered in multiply\n","  self.moving_variance = self.alpha * (self.moving_variance + self.EPS) + (1 - self.alpha) * self.batch_var\n"]}],"source":["%cd /content/drive/MyDrive/DL/\n","%run homework_modules.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncR3rbxJqcd-"},"outputs":[],"source":["suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n","unittest.TextTestRunner(verbosity=2).run(suite)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWzvCWoMMQ-Z"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"}},"nbformat":4,"nbformat_minor":0}